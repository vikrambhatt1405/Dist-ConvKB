import tensorflow as tf
import numpy as np

np.random.seed(1234)
import os
import time
import datetime
from argparse import ArgumentParser, ArgumentDefaultsHelpFormatter
from builddata import *
from downpour_model import ConvKB
from tf_graph_functions import *
REPLICAS_TO_AGGREGATE=2
# Parameters
# ==================================================
parser = ArgumentParser("CapsE", formatter_class=ArgumentDefaultsHelpFormatter, conflict_handler='resolve')

parser.add_argument("--data", default="./data/", help="Data sources.")
parser.add_argument("--run_folder", default="./", help="Data sources.")
parser.add_argument("--name", default="WN18RR", help="Name of the dataset.")

parser.add_argument("--embedding_dim", default=50, type=int, help="Dimensionality of character embedding")
parser.add_argument("--filter_sizes", default="1", help="Comma-separated filter sizes")
parser.add_argument("--num_filters", default=500, type=int, help="Number of filters per filter size")
parser.add_argument("--dropout_keep_prob", default=1.0, type=float, help="Dropout keep probability")
parser.add_argument("--l2_reg_lambda", default=0.001, type=float, help="L2 regularization lambda")
parser.add_argument("--learning_rate", default=0.0001, type=float, help="Learning rate")
parser.add_argument("--is_trainable", default=True, type=bool, help="")
parser.add_argument("--batch_size", default=128, type=int, help="Batch Size")
parser.add_argument("--neg_ratio", default=1.0, type=float, help="Number of negative triples generated by positive")
parser.add_argument("--use_pretrained", default=True, type=bool, help="Using the pretrained embeddings")
parser.add_argument("--num_epochs", default=201, type=int, help="Number of training epochs")
parser.add_argument("--saveStep", default=200, type=int, help="")
parser.add_argument("--allow_soft_placement", default=True, type=bool, help="Allow device soft device placement")
parser.add_argument("--log_device_placement", default=False, type=bool, help="Log placement of ops on devices")
parser.add_argument("--model_name", default='wn18rr', help="")
parser.add_argument("--useConstantInit", action='store_true')

parser.add_argument("--model_index", default='200', help="")
parser.add_argument("--num_splits", default=8, type=int, help="Split the validation set into 8 parts for a faster evaluation")
parser.add_argument("--testIdx", default=1, type=int, help="From 0 to 7. Index of one of 8 parts")
parser.add_argument("--decode", action='store_false')
parser.add_argument("--update_window",type=int,default=3,help="Specify update window for DOWNPOUR")
parser.add_argument("--ps_hosts",default="",help="Comma-separated list of hostname:port pairs")
parser.add_argument("--worker_hosts",default="",help="Comma-separated list of hostname:port pairs")
parser.add_argument("--job_name",default="",help="One of 'ps', 'worker'")
parser.add_argument("--task_index",default=0,type=int,help= "Index of task within the job")

args = parser.parse_args()
print(args)
config=tf.ConfigProto(log_device_placement=False)
ps_hosts=args.ps_hosts.split(",")
worker_hosts=args.worker_hosts.split(",")
cluster = tf.train.ClusterSpec({"ps": ps_hosts, "worker": worker_hosts})
server = tf.train.Server(cluster,job_name=args.job_name,task_index=args.task_index)
n_pss = len(ps_hosts)
n_workers =  len(worker_hosts)
if args.job_name == 'ps':
    server.join()
elif args.job_name == 'worker':
    print("Loading data...")
    is_chief = (args.task_index == 0)
    train, valid, test, words_indexes, indexes_words, \
    headTailSelector, entity2id, id2entity, relation2id, id2relation = build_data(path=args.data, name=args.name)
    data_size = len(train)
    train_batch = Batch_Loader(train, words_indexes, indexes_words, headTailSelector, \
                           entity2id, id2entity, relation2id, id2relation, batch_size=args.batch_size,
                           neg_ratio=args.neg_ratio)

    entity_array = np.array(list(train_batch.indexes_ents.keys()))

    lstEmbed = []
    if args.use_pretrained == True:
        print("Using pre-trained model.")
        lstEmbed = np.empty([len(words_indexes), args.embedding_dim]).astype(np.float32)
        initEnt, initRel = init_norm_Vector(args.data + args.name + '/relation2vec' + str(args.embedding_dim) + '.init',
                                        args.data + args.name + '/entity2vec' + str(args.embedding_dim) + '.init',
                                        args.embedding_dim)
        for _word in words_indexes:
            if _word in relation2id:
                index = relation2id[_word]
                _ind = words_indexes[_word]
                lstEmbed[_ind] = initRel[index]
            elif _word in entity2id:
                index = entity2id[_word]
                _ind = words_indexes[_word]
                lstEmbed[_ind] = initEnt[index]
            else:
                print('*****************Error********************!')
                break
        lstEmbed = np.array(lstEmbed, dtype=np.float32)

    assert len(words_indexes) % (len(entity2id) + len(relation2id)) == 0

    print("Loading data... finished!")

    x_valid = np.array(list(valid.keys())).astype(np.int32)
    y_valid = np.array(list(valid.values())).astype(np.float32)

    x_test = np.array(list(test.keys())).astype(np.int32)
    y_test = np.array(list(test.values())).astype(np.float32)

# Training
# ==================================================
    with tf.Graph().as_default():
        tf.set_random_seed(1234)
        session_conf = tf.ConfigProto(allow_soft_placement=args.allow_soft_placement,
                                  log_device_placement=args.log_device_placement)
        session_conf.gpu_options.allow_growth = True
        with tf.device("/job:worker/replica:0/task:%d" % args.task_index):
            local_step = tf.Variable(0,dtype=tf.int32,trainable=False,collections=['local_non_trainable'])

            cnn = ConvKB(
                sequence_length=x_valid.shape[1],  # 3
                num_classes=y_valid.shape[1],  # 1
                pre_trained=lstEmbed,
                embedding_size=args.embedding_dim,
                filter_sizes=list(map(int, args.filter_sizes.split(","))),
                num_filters=args.num_filters,
                vocab_size=len(words_indexes),
                l2_reg_lambda=args.l2_reg_lambda,
                batch_size=(int(args.neg_ratio) + 1) * args.batch_size,
                is_trainable=args.is_trainable,
                useConstantInit=args.useConstantInit)


            local_optimizer = tf.train.AdamOptimizer(learning_rate=args.learning_rate)
            update_window=args.update_window
            grads_list=[]
            print(tf.local_variables())
            for t in range(update_window):
                if t != 0:
                    with tf.control_dependencies([opt_local]): #compute gradients only if the local opt was run
                        grads, varss = zip(*local_optimizer.compute_gradients( \
									cnn.loss,var_list=tf.local_variables()))
                else:
                    grads, varss = zip(*local_optimizer.compute_gradients( \
								cnn.loss,var_list=tf.local_variables()))
                grads_list.append(grads) #add gradients to the list
                opt_local = local_optimizer.apply_gradients(zip(grads,varss),
							global_step=local_step) #update local parameters
            for i in grads_list:
                print(i)
            grads_list = tf.convert_to_tensor(grads_list)
            grads = tf.reduce_sum(grads_list,axis=0) #sum updates before applying globally
            grads = tuple([grads[i]for i in range(len(varss))])

			# add these variables created by local optimizer to local collection
            lopt_vars = add_global_variables_to_local_collection()

			# delete the variables from the global collection
            clear_global_collection()
        # optimizer = tf.train.RMSPropOptimizer(learning_rate=args.learning_rate)
        # optimizer = tf.train.GradientDescentOptimizer (learning_rate=args.learning_rate)

        with tf.device(tf.train.replica_device_setter(ps_tasks=n_pss,worker_device="/job:%s/task:%d" % (args.job_name,args.task_index))):
            global_step = tf.Variable(0,dtype=tf.int32,trainable=False,name='global_step')

			# all workers use the same learning rate and it is decided on by the task 0
			# or maybe the from the graph of the chief worker
            optimizer = tf.train.AdagradOptimizer(args.learning_rate) #global optimizer

			# create global variables and/or references
            local_to_global, global_to_local = create_global_variables(lopt_vars)
            opt = optimizer.apply_gradients(
						zip(grads,[local_to_global[v] for v in varss])
						,global_step=global_step) #apply the gradients to variables on ps

			# Pull params from global server
            with tf.control_dependencies([opt]):
                assign_locals = assign_global_to_local(global_to_local)

			# Grab global state before training so all workers have same initialization
            grab_global_init = assign_global_to_local(global_to_local)

			# Assigns local values to global ones for chief to execute
            assign_global = assign_local_to_global(local_to_global)

			# Init ops
            init = tf.global_variables_initializer() # for global variables
            init_local = tf.variables_initializer(tf.local_variables() \
						+tf.get_collection('local_non_trainable')) #for local variables
        out_dir = os.path.abspath(os.path.join(args.run_folder, "runs", args.model_name,str(args.task_index)))
        print("Writing to {}\n".format(out_dir))
        tflogs_dir=os.path.abspath(os.path.join(out_dir,"tf_logs"))
        print("Creating tensorflow logs at {}\n".format(tflogs_dir))
        summary_hook = tf.train.SummarySaverHook(save_steps=50,output_dir=tflogs_dir,
                                                 summary_op=cnn.loss_summary)
        hooks=[tf.train.StopAtStepHook(last_step=1000000),summary_hook]
        #file_writer = tf.summary.FileWriter(out_dir+"tf_logs", tf.get_default_graph())

        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it
        checkpoint_dir = os.path.abspath(os.path.join(out_dir, "checkpoints"))
        checkpoint_prefix = os.path.join(checkpoint_dir, "model")
        if not os.path.exists(checkpoint_dir):
            os.makedirs(checkpoint_dir)

        # Initialize all variables
        scaff = tf.train.Scaffold(init_op=init,local_init_op=[init_local])
        sess = tf.train.MonitoredTrainingSession(master=server.target,
        				is_chief=(args.task_index == 0),config=config,scaffold=scaff,
                        hooks=hooks,save_checkpoint_steps=args.saveStep)
        if is_chief:
            sess.run(assign_global) #Assigns chief's initial values to ps


        def train_step(x_batch, y_batch):
            """
            A single training step
            """
            feed_dict = {
                cnn.input_x: x_batch,
                cnn.input_y: y_batch,
                cnn.dropout_keep_prob: args.dropout_keep_prob,}
            _,_,loss,gs,ls = sess.run([opt,assign_locals,cnn.loss,global_step,local_step],feed_dict)
            return loss,gs,ls

        num_batches_per_epoch = int((data_size - 1) / args.batch_size) + 1
        while not sess.should_stop():
            x_batch,y_batch=train_batch()
            loss,gs,ls=train_step(x_batch,y_batch)
            print(loss,"global step: "+str(gs),"worker: "+str(args.task_index),"local step: "+str(ls))
            time.sleep(1) # so we can observe training
            #for epoch in range(args.num_epochs):
                #for batch_num in range(num_batches_per_epoch):
                    #x_batch, y_batch = train_batch()
                    #step,loss= train_step(x_batch, y_batch)
                    #current_step = tf.train.global_step(sess, global_step)
                #if epoch > 0:
                    #if epoch % args.saveStep == 0:
                        #path = cnn.saver.save(sess, checkpoint_prefix, global_step=epoch)
                        #print("Saved model checkpoint to {}\n".format(path))
        sess.close()
        #print('Session from worker %d closed cleanly'%args.task_index)
